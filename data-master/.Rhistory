# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
#################################3
bag.ovr <- makeBaggingWrapper(ovr, bw.iters = 10, #default of 10
bw.replace = TRUE, #default
bw.size = 0.7,
bw.feats = 1)
#bag.ovr <- setPredictType(bag.ovr, predict.type = "prob")
#################################################################
set.seed(317)
fitOVR <- mlr::train(bag.ovr, wine.task)
predOVR <- predict(fitOVR, newdata = test)
head(data.frame(predOVR))
getConfMatrix(predOVR)
# wrapper
#models <- list(makeLearner("classif.rpart", predict.type = "prob"),
#               makeLearner("classif.glmnet", predict.type = "prob"))
pima.task <- makeClassifTask(id = "pima", data = train, target = "type")
pima.smote <- smote(pima.task, rate = 2, nn = 3)
str(getTaskData(pima.smote))
base <- c("classif.randomForest", "classif.qda", "classif.glmnet")
learns <- lapply(base, makeLearner)
learns <- lapply(learns, setPredictType, "prob")
sl <- makeStackedLearner(base.learners = learns,
super.learner = "classif.logreg",
predict.type = "prob",
method = "stack.cv")
slFit <- mlr::train(sl, pima.smote)
predFit <- predict(slFit, newdata = test)
getConfMatrix(predFit)
performance(predFit, measures = list(mmce, acc, auc))
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
# wrapper
#models <- list(makeLearner("classif.rpart", predict.type = "prob"),
#               makeLearner("classif.glmnet", predict.type = "prob"))
pima.task <- makeClassifTask(id = "pima", data = train, target = "type")
#bag.ovr <- setPredictType(bag.ovr, predict.type = "prob")
#################################################################
set.seed(317)
fitOVR <- mlr::train(bag.ovr, wine.task)
predOVR <- predict(fitOVR, newdata = test)
head(data.frame(predOVR))
install.packages('ggfortify')
install.packages('forecast')
library(tseries)
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
ar1
autoplot(ar1, main = "AR1")
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
ar1
autoplot(ar1, main = "AR1")
# install.packages('ggfortify')
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
ar1 <-as.data.frame(ar1)
View(ar1)
autoplot(ar1, main = "AR1")
ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
View(ar1)
View(ar1)
autoplot(ar1$x, main = "AR1")
autoplot(ar1$x, main = "AR1")
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
ar1
# ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
# install.packages('ggfortify')
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
autoplot(pacf(ar1, plot = F), main = "AR1 - PACF")
set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
autoplot(ma1, main = "MA1")
autoplot(acf(ma1, plot = F), main = "MA1 - ACF")
autoplot(pacf(ma1, plot = F), main = "MA1 - PACF")
climate <- read.csv("climate.csv", stringsAsFactors = F)
View(climate)
str(climate)
climate <- ts(climate[, 2:3], frequency = 1,
start = 1919, end = 2013)
View(climate)
head(climate)
# install.packages('forecast')
library(forecast)
library(tseries)
plot(climate)
View(climate)
cor(climate)
autoplot(acf(climate[, 2], plot = F), main="Temp ACF")
autoplot(pacf(climate[, 2], plot = F), main="Temp PACF")
autoplot(acf(climate[, 1], plot = F), main="CO2 ACF")
autoplot(pacf(climate[, 1], plot = F), main = "CO2 PACF")
climate <- read.csv("climate.csv", stringsAsFactors = F)
ccf(climate[, 1], climate[, 2], main = "CCF")
adf.test(climate[, 1])
adf.test(climate[, 2])
View(climate)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
autoplot(pacf(ar1, plot = F), main = "AR1 - PACF")
set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
autoplot(ma1, main = "MA1")
autoplot(acf(ma1, plot = F), main = "MA1 - ACF")
autoplot(pacf(ma1, plot = F), main = "MA1 - PACF")
climate <- read.csv("climate.csv", stringsAsFactors = F)
str(climate)
climate <- ts(climate[, 2:3], frequency = 1,
start = 1919, end = 2013)
head(climate)
# install.packages('forecast')
library(forecast)
library(tseries)
plot(climate)
View(climate)
cor(climate)
autoplot(acf(climate[, 2], plot = F), main="Temp ACF")
autoplot(pacf(climate[, 2], plot = F), main="Temp PACF")
autoplot(acf(climate[, 1], plot = F), main="CO2 ACF")
autoplot(pacf(climate[, 1], plot = F), main = "CO2 PACF")
ccf(climate[, 1], climate[, 2], main = "CCF")
adf.test(climate[, 1])
adf.test(climate[, 2])
temp <- climate[, 2]
temp <- climate[, 2]
train <- window(temp, start = 1946, end = 2003)
test <- window(temp, start = 2004)
train
test
fit.holt <- holt(train, h = 10, initial = "optimal")
View(fit.holt)
# summary(fit.holt)
plot(forecast(fit.holt))
lines(test, type= "o")
fit.holtd <- holt(train, h = 10, initial = "optimal", damped = TRUE)
# summary(fit.holtd)
plot(forecast(fit.holtd), main ="Holt Damped")
lines(test, type = "o")
fit.arima <- auto.arima(train)
summary(fit.arima)
plot(forecast(fit.arima, h = 10))
lines(test, type="o")
mapeHOLT <- sum(abs((test - fit.holt$mean)/test))/10
mapeHOLT
mapeHOLTD <- sum(abs((test - fit.holtd$mean)/test))/10
mapeHOLTD
mapeARIMA <- sum(abs((test - forecast(fit.arima, h = 10)$mean)/test))/10
mapeARIMA
fit.lm <- lm(Temp ~ CO2, data = climate)
summary(fit.lm)
plot.ts(fit.lm$residuals)
acf(fit.lm$residuals)
dwtest(fit.lm)
View(fit.lm)
acf(fit.lm$residuals)
dwtest(fit.lm)
ndiffs(climate[, 1], test = "adf")
ndiffs(climate[, 2], test = "adf")
library(vars)
install.packages('vars')
install.packages('aod')
# install.packages('aod')
library(vars)
library(aod)
climateDiff <- diff(climate)
View(climateDiff)
View(climate)
View(climateDiff)
View(climate)
View(climateDiff)
climateDiff <- window(climateDiff, start = 1946)
head(climateDiff)
lag.select <- VARselect(climateDiff, lag.max = 12)
View(lag.select)
lag.select$selection
fit1 <- VAR(climateDiff, p = 5)
summary(fit1)
serial.test(fit1, type = "PT.asymptotic")
x2y <- causality(fit1, cause = "CO2")
View(x2y)
y2x <- causality(fit1, cause = "Temp")
x2y$Granger
y2x$Granger
climateLevels <- window(climate, start = 1946)
View(climateLevels)
level.select <- VARselect(climateLevels, lag.max = 12)
level.select$selection
fit2 <- VAR(climateLevels, p = 7)
serial.test(fit2, type = "BG")
CO2terms <- seq(1, 11, 2)
Tempterms <- seq(2, 12, 2)
CO2terms
Tempterms
wald.test(b = coef(fit2$varresult$Temp),
Sigma = vcov(fit2$varresult$Temp),
Terms = c(CO2terms))
wald.test(b = coef(fit2$varresult$CO2),
Sigma = vcov(fit2$varresult$CO2),
Terms = c(Tempterms))
autoplot(predict(fit2, n.ahead=25, ci=0.95))
library(tm)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
library(NLP)
install.packages('NLP')
install.packages("NLP")
library(tm)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(NLP)
library(NLP)
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
View(docs)
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
# docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
rownames(dtm) <- c("2010","2011","2012","2013","2014","2015","2016")
inspect(dtm[1:7, 1:5])
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
# docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
# docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
rownames(dtm) <- c("2010","2011","2012","2013","2014","2015","2016")
inspect(dtm[1:7, 1:5])
freq = colSums(as.matrix(dtm))
ord = order(-freq) #order the frequency
freq[head(ord)]
freq[tail(ord)]
head(table(freq))
tail(table(freq))
findFreqTerms(dtm, 125)
findAssocs(dtm, "jobs", corlimit = 0.85)
wordcloud(names(freq), freq,
min.freq = 70, scale = c(3, .5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, max.words = 25)
freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
wf <- data.frame(word = names(freq), freq = freq)
wf <- wf[1:10, ]
barplot(wf$freq, names = wf$word, main = "Word Frequency",
xlab = "Words", ylab = "Counts", ylim = c(0, 250))
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
# docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
# docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
inspect(dtm[1:7, 1:5])
rownames(dtm) <- c("2010","2011","2012","2013","2014","2015","2016")
freq = colSums(as.matrix(dtm))
ord = order(-freq) #order the frequency
freq[head(ord)]
freq[tail(ord)]
head(table(freq))
tail(table(freq))
findFreqTerms(dtm, 125)
findAssocs(dtm, "jobs", corlimit = 0.85)
wordcloud(names(freq), freq,
min.freq = 70, scale = c(3, .5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, max.words = 25)
freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
freq
wf <- data.frame(word = names(freq), freq = freq)
View(wf)
wf <- wf[1:10, ]
barplot(wf$freq, names = wf$word, main = "Word Frequency",
xlab = "Words", ylab = "Counts", ylim = c(0, 250))
#topic models
library(topicmodels)
install.packages('topicmodels')
#topic models
library(topicmodels)
set.seed(123)
lda3 <- LDA(dtm, k = 3, method = "Gibbs")
View(lda3)
#topic models
library(topicmodels)
set.seed(123)
lda3 <- LDA(dtm, k = 3, method = "Gibbs")
View(lda3)
lda3 <- LDA(dtm, k = 3, method = "Gibbs")
topics(lda3)
set.seed(456)
terms(lda3, 25)
library(qdap)
install.packages('qdap')
library(qdap)
library(qdap)
speech16 <- paste(readLines("sou2016.txt"), collapse=" ")
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
speech16 <- paste(readLines("sou2016.txt"), collapse=" ")
speech16
speech16 <- paste(readLines("sou2016.txt"), collapse=" ")
speech16 <- iconv(speech16, "latin1", "ASCII", "")
prep16 <- qprep(speech16)
prep16 <- replace_contraction(prep16)
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
# docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
# docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
rownames(dtm) <- c("2010","2011","2012","2013","2014","2015","2016")
inspect(dtm[1:7, 1:5])
freq = colSums(as.matrix(dtm))
ord = order(-freq) #order the frequency
freq[head(ord)]
freq[tail(ord)]
head(table(freq))
tail(table(freq))
findFreqTerms(dtm, 125)
findAssocs(dtm, "jobs", corlimit = 0.85)
wordcloud(names(freq), freq,
min.freq = 70, scale = c(3, .5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, max.words = 25)
freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
wf <- data.frame(word = names(freq), freq = freq)
wf <- wf[1:10, ]
barplot(wf$freq, names = wf$word, main = "Word Frequency",
xlab = "Words", ylab = "Counts", ylim = c(0, 250))
#topic models
library(topicmodels)
set.seed(123)
lda3 <- LDA(dtm, k = 3, method = "Gibbs")
topics(lda3)
set.seed(456)
terms(lda3, 25)
library(qdap)
library(qdap)
install.packages('qdap')
library(qdap)
