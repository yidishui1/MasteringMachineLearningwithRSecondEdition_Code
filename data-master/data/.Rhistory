base <- c("classif.randomForest", "classif.qda", "classif.glmnet")
learns <- lapply(base, makeLearner)
learns <- lapply(learns, setPredictType, "prob")
sl <- makeStackedLearner(base.learners = learns,
super.learner = "classif.logreg",
predict.type = "prob",
method = "stack.cv")
slFit <- mlr::train(sl, pima.smote)
predFit <- predict(slFit, newdata = test)
getConfMatrix(predFit)
performance(predFit, measures = list(mmce, acc, auc))
pima <- rbind(Pima.tr, Pima.te)
set.seed(502)
split <- createDataPartition(y = pima$type, p = 0.75, list = F)
View(split)
View(split)
train <- pima[split, ]
test <- pima[-split, ]
table(train$type)
View(train)
control <- trainControl(method = "cv",
number = 5,
savePredictions = "final",
classProbs = T,
index=createResample(train$type, 5),
sampling = "up",
summaryFunction = twoClassSummary)
set.seed(2)
models <- caretList(
type ~ ., data = train,
trControl = control,
metric = "ROC",
methodList = c("rpart", "earth", "knn")
)
models
# xyplot(resamples(models))
modelCor(resamples(models))
model_preds <- lapply(models, predict, newdata=test, type="prob")
View(model_preds)
View(model_preds[["rpart"]])
View(model_preds[["earth"]])
model_preds <- lapply(model_preds, function(x) x[,"Yes"])
View(model_preds)
View(model_preds[["rpart"]])
View(model_preds[["earth"]])
model_preds <- data.frame(model_preds)
View(model_preds)
stack <- caretStack(models, method = "glm",
metric = "ROC",
trControl = trainControl(
method = "boot",
number = 5,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
))
summary(stack)
# rawPreds <- predict(stack, newdata = test, type = "raw")
prob <- 1-predict(stack, newdata = test, type = "prob")
prob
model_preds$ensemble <- prob
View(model_preds)
View(test)
colAUC(model_preds, test$type)
# mlR
# install.packages('DMwR')
# install.packages('corrplot')
library(mlr)
library(ggplot2)
library(HDclassif)
library(DMwR)
library(reshape2)
library(corrplot)
data(wine)
table(wine$class)
wine$class <- as.factor(wine$class)
View(wine)
set.seed(11)
df <- SMOTE(class ~ ., wine, perc.over = 300, perc.under = 300)
table(df$class)
View(df)
View(wine)
wine.scale <- data.frame(scale(wine[, 2:5]))
View(wine.scale)
wine.scale$class <- wine$class
wine.melt = melt(wine.scale, id.var="class")
View(wine.melt)
ggplot(data = wine.melt, aes( x = class, y = value)) +
geom_boxplot() +
facet_wrap( ~ variable, ncol = 2)
#outliers
outHigh <- function(x) {
x[x > quantile(x, 0.99)] <- quantile(x, 0.9)
x
}
outLow <- function(x) {
x[x < quantile(x, 0.01)] <- quantile(x, 0.1)
x
}
wine.trunc <- data.frame(lapply(wine[, -1], outHigh))
View(wine.trunc)
wine.trunc <- data.frame(lapply(wine.trunc, outLow))
wine.trunc$class <- wine$class
boxplot(wine.trunc$V3 ~ wine.trunc$class)
outHigh
c <- cor(wine.trunc[, -14])
corrplot.mixed(c, upper = "ellipse")
library(caret) #if not already loaded
set.seed(502)
split <- createDataPartition(y = df$class, p = 0.7, list = F)
train <- df[split, ]
test <- df[-split, ]
wine.task <- makeClassifTask(id = "wine", data = train,
target = "class")
View(wine.task)
# wine.task <- normalizeFeatures(wine.task, method = "standardize")
str(getTaskData(wine.task))
rdesc <- makeResampleDesc("Subsample", iters = 3)
rdesc
View(rdesc)
View(rdesc)
param <- makeParamSet(
makeDiscreteParam("ntree", values = c(750, 1000, 1250, 1500, 1750, 2000))
)
ctrl <- makeTuneControlGrid()
View(ctrl)
# list of models
# https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html
tuning <- tuneParams("classif.randomForest", task = wine.task,
resampling = rdesc, par.set = param,
control = ctrl)
tuning$x
tuning$y
# list of models
# https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html
tuning <- tuneParams("classif.randomForest", task = wine.task,
resampling = rdesc, par.set = param,
control = ctrl)
tuning$x
tuning$y
rf <- setHyperPars(makeLearner("classif.randomForest",
predict.type = "prob"), par.vals = tuning$x)
# getHyperPars(rf)
fitRF <- train(rf, wine.task)
fitRF$learner.model
predRF <- predict(fitRF, newdata = test)
# head(data.frame(predRF))
getConfMatrix(predRF)
# head(data.frame(predRF))
# getConfMatrix(predRF)
calculateConfusionMatrix(predRF)
performance(predRF, measures = list(mmce, acc))
# install.packages('caretEnsemble')
install.packages('penalized')
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
# install.packages('caretEnsemble')
install.packages('penalized')
library(penalized)
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
#################################3
bag.ovr <- makeBaggingWrapper(ovr, bw.iters = 10, #default of 10
bw.replace = TRUE, #default
bw.size = 0.7,
bw.feats = 1)
#bag.ovr <- setPredictType(bag.ovr, predict.type = "prob")
#################################################################
set.seed(317)
fitOVR <- mlr::train(bag.ovr, wine.task)
predOVR <- predict(fitOVR, newdata = test)
head(data.frame(predOVR))
getConfMatrix(predOVR)
# wrapper
#models <- list(makeLearner("classif.rpart", predict.type = "prob"),
#               makeLearner("classif.glmnet", predict.type = "prob"))
pima.task <- makeClassifTask(id = "pima", data = train, target = "type")
pima.smote <- smote(pima.task, rate = 2, nn = 3)
str(getTaskData(pima.smote))
base <- c("classif.randomForest", "classif.qda", "classif.glmnet")
learns <- lapply(base, makeLearner)
learns <- lapply(learns, setPredictType, "prob")
sl <- makeStackedLearner(base.learners = learns,
super.learner = "classif.logreg",
predict.type = "prob",
method = "stack.cv")
slFit <- mlr::train(sl, pima.smote)
predFit <- predict(slFit, newdata = test)
getConfMatrix(predFit)
performance(predFit, measures = list(mmce, acc, auc))
# classif.penalized.ridge
# one vs rest
ovr <- makeMulticlassWrapper("classif.penalized.ridge", mcw.method = "onevsrest")
# wrapper
#models <- list(makeLearner("classif.rpart", predict.type = "prob"),
#               makeLearner("classif.glmnet", predict.type = "prob"))
pima.task <- makeClassifTask(id = "pima", data = train, target = "type")
#bag.ovr <- setPredictType(bag.ovr, predict.type = "prob")
#################################################################
set.seed(317)
fitOVR <- mlr::train(bag.ovr, wine.task)
predOVR <- predict(fitOVR, newdata = test)
head(data.frame(predOVR))
install.packages('ggfortify')
install.packages('forecast')
library(tseries)
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
ar1
autoplot(ar1, main = "AR1")
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
ar1
autoplot(ar1, main = "AR1")
# install.packages('ggfortify')
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
ar1 <-as.data.frame(ar1)
View(ar1)
autoplot(ar1, main = "AR1")
ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
View(ar1)
View(ar1)
autoplot(ar1$x, main = "AR1")
autoplot(ar1$x, main = "AR1")
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
ar1
# ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
# install.packages('ggfortify')
library(ggfortify)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
autoplot(pacf(ar1, plot = F), main = "AR1 - PACF")
set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
autoplot(ma1, main = "MA1")
autoplot(acf(ma1, plot = F), main = "MA1 - ACF")
autoplot(pacf(ma1, plot = F), main = "MA1 - PACF")
climate <- read.csv("climate.csv", stringsAsFactors = F)
View(climate)
str(climate)
climate <- ts(climate[, 2:3], frequency = 1,
start = 1919, end = 2013)
View(climate)
head(climate)
# install.packages('forecast')
library(forecast)
library(tseries)
plot(climate)
View(climate)
cor(climate)
autoplot(acf(climate[, 2], plot = F), main="Temp ACF")
autoplot(pacf(climate[, 2], plot = F), main="Temp PACF")
autoplot(acf(climate[, 1], plot = F), main="CO2 ACF")
autoplot(pacf(climate[, 1], plot = F), main = "CO2 PACF")
climate <- read.csv("climate.csv", stringsAsFactors = F)
ccf(climate[, 1], climate[, 2], main = "CCF")
adf.test(climate[, 1])
adf.test(climate[, 2])
View(climate)
set.seed(123)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# ar1 <-as.data.frame(ar1)
autoplot(ar1, main = "AR1")
autoplot(acf(ar1, plot = F), main = "AR1 - ACF")
autoplot(pacf(ar1, plot = F), main = "AR1 - PACF")
set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
autoplot(ma1, main = "MA1")
autoplot(acf(ma1, plot = F), main = "MA1 - ACF")
autoplot(pacf(ma1, plot = F), main = "MA1 - PACF")
climate <- read.csv("climate.csv", stringsAsFactors = F)
str(climate)
climate <- ts(climate[, 2:3], frequency = 1,
start = 1919, end = 2013)
head(climate)
# install.packages('forecast')
library(forecast)
library(tseries)
plot(climate)
View(climate)
cor(climate)
autoplot(acf(climate[, 2], plot = F), main="Temp ACF")
autoplot(pacf(climate[, 2], plot = F), main="Temp PACF")
autoplot(acf(climate[, 1], plot = F), main="CO2 ACF")
autoplot(pacf(climate[, 1], plot = F), main = "CO2 PACF")
ccf(climate[, 1], climate[, 2], main = "CCF")
adf.test(climate[, 1])
adf.test(climate[, 2])
temp <- climate[, 2]
temp <- climate[, 2]
train <- window(temp, start = 1946, end = 2003)
test <- window(temp, start = 2004)
train
test
fit.holt <- holt(train, h = 10, initial = "optimal")
View(fit.holt)
# summary(fit.holt)
plot(forecast(fit.holt))
lines(test, type= "o")
fit.holtd <- holt(train, h = 10, initial = "optimal", damped = TRUE)
# summary(fit.holtd)
plot(forecast(fit.holtd), main ="Holt Damped")
lines(test, type = "o")
fit.arima <- auto.arima(train)
summary(fit.arima)
plot(forecast(fit.arima, h = 10))
lines(test, type="o")
mapeHOLT <- sum(abs((test - fit.holt$mean)/test))/10
mapeHOLT
mapeHOLTD <- sum(abs((test - fit.holtd$mean)/test))/10
mapeHOLTD
mapeARIMA <- sum(abs((test - forecast(fit.arima, h = 10)$mean)/test))/10
mapeARIMA
fit.lm <- lm(Temp ~ CO2, data = climate)
summary(fit.lm)
plot.ts(fit.lm$residuals)
acf(fit.lm$residuals)
dwtest(fit.lm)
View(fit.lm)
acf(fit.lm$residuals)
dwtest(fit.lm)
ndiffs(climate[, 1], test = "adf")
ndiffs(climate[, 2], test = "adf")
library(vars)
install.packages('vars')
install.packages('aod')
# install.packages('aod')
library(vars)
library(aod)
climateDiff <- diff(climate)
View(climateDiff)
View(climate)
View(climateDiff)
View(climate)
View(climateDiff)
climateDiff <- window(climateDiff, start = 1946)
head(climateDiff)
lag.select <- VARselect(climateDiff, lag.max = 12)
View(lag.select)
lag.select$selection
fit1 <- VAR(climateDiff, p = 5)
summary(fit1)
serial.test(fit1, type = "PT.asymptotic")
x2y <- causality(fit1, cause = "CO2")
View(x2y)
y2x <- causality(fit1, cause = "Temp")
x2y$Granger
y2x$Granger
climateLevels <- window(climate, start = 1946)
View(climateLevels)
level.select <- VARselect(climateLevels, lag.max = 12)
level.select$selection
fit2 <- VAR(climateLevels, p = 7)
serial.test(fit2, type = "BG")
CO2terms <- seq(1, 11, 2)
Tempterms <- seq(2, 12, 2)
CO2terms
Tempterms
wald.test(b = coef(fit2$varresult$Temp),
Sigma = vcov(fit2$varresult$Temp),
Terms = c(CO2terms))
wald.test(b = coef(fit2$varresult$CO2),
Sigma = vcov(fit2$varresult$CO2),
Terms = c(Tempterms))
autoplot(predict(fit2, n.ahead=25, ci=0.95))
library(SnowballC)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master")
name <- file.path("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
length(dir(name))
dir(name)
docs <- Corpus(DirSource(name))
docs
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
# docs <- tm_map(docs, stemDocument, language = "porter")
docs <- tm_map(docs, removeWords, c("applause", "can", "cant", "will", "that",
"weve", "dont","wont", "youll", "youre",
"thats"))
# docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
rownames(dtm) <- c("2010","2011","2012","2013","2014","2015","2016")
inspect(dtm[1:7, 1:5])
freq = colSums(as.matrix(dtm))
ord = order(-freq) #order the frequency
freq[head(ord)]
freq[tail(ord)]
head(table(freq))
tail(table(freq))
findFreqTerms(dtm, 125)
findAssocs(dtm, "jobs", corlimit = 0.85)
wordcloud(names(freq), freq,
min.freq = 70, scale = c(3, .5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, max.words = 25)
freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
wf <- data.frame(word = names(freq), freq = freq)
wf <- wf[1:10, ]
barplot(wf$freq, names = wf$word, main = "Word Frequency",
xlab = "Words", ylab = "Counts", ylim = c(0, 250))
#topic models
library(topicmodels)
set.seed(123)
lda3 <- LDA(dtm, k = 3, method = "Gibbs")
topics(lda3)
set.seed(456)
terms(lda3, 25)
library(qdap)
speech16 <- paste(readLines("sou2016.txt"), collapse=" ")
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
speech16 <- paste(readLines("sou2016.txt"), collapse=" ")
speech16 <- iconv(speech16, "latin1", "ASCII", "")
prep16 <- qprep(speech16)
prep16 <- replace_contraction(prep16)
prep16 <- rm_stopwords(prep16, Top100Words, separate = F)
prep16 <- strip(prep16, char.keep = c("?", "."))
sent16 <- data.frame(speech = prep16)
sent16 <- sentSplit(sent16, "speech")
sent16$year <- "2016"
speech10 <- paste(readLines("sou2010.txt"), collapse=" ")
speech10 <- iconv(speech10, "latin1", "ASCII", "")
speech10 <- gsub("(Applause.)", "", speech10)
prep10 <- qprep(speech10)
prep10 <- replace_contraction(prep10)
prep10 <- rm_stopwords(prep10, Top100Words, separate = F)
prep10 <- strip(prep10, char.keep = c("?", "."))
sent10 <- data.frame(speech = prep10)
sent10 <- sentSplit(sent10, "speech")
sent10$year <- "2010"
sentences <- data.frame(rbind(sent10, sent16))
plot(freq_terms(sentences$speech))
wordMat <- wfm(sentences$speech, sentences$year)
head(wordMat[order(wordMat[, 1], wordMat[, 2],decreasing = TRUE),])
trans_cloud(sentences$speech, sentences$year, min.freq = 10)
ws <- word_stats(sentences$speech, sentences$year, rm.incomplete = T)
plot(ws, label = T, lab.digits = 2)
pol <- polarity(text.var = sentences$speech,
grouping.var = sentences$year)
pol
plot(pol)
pol.df <- pol$all
which.min(pol.df$polarity)
pol.df$text.var[12]
ari <- automated_readability_index(text.var = sentences$speech,
grouping.var = sentences$year)
ari$Readability
form <- formality(sentences$speech, sentences$year)
form
form$form.prop.by
plot(form)
div <- diversity(sentences$speech, sentences$year)
div
dispersion_plot(sentences$speech,
rm.vars = sentences$year,
c("security", "jobs", "economy"),
color = "black", bg.color = "white")
library(qdap)
setwd("C:/Users/leishen/Documents/R/win-library/MasteringMachineLearningwithRSecondEdition_Code/data-master/data")
speech16 <- paste(readLines("sou2016.txt"), collapse=" ")
speech16 <- iconv(speech16, "latin1", "ASCII", "")
prep16 <- qprep(speech16)
prep16 <- replace_contraction(prep16)
install.packages('qprep')
library(qdap)
library(qdap)
library("rJava")
library(qdap)
library("rJava")
library('qdap')
library(qdap)
library(qdap)
library("rJava")
library(qdap)
